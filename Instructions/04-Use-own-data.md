---
lab:
  title: 独自のデータを使用する生成 AI アプリを作成する
  description: 検索拡張生成 (RAG) モデルを使用して、独自のデータを使ってプロンプトをグラウンディングするチャット アプリを構築する方法について説明します。
---

# 独自のデータを使用する生成 AI アプリを作成する

取得拡張生成 (RAG) は、カスタム データ ソースからのデータを、生成 AI モデルのプロンプトに統合するアプリケーションを構築するために使用される手法です。 RAG は、生成 AI アプリを開発するために一般的に使用されるパターンです。チャット ベースのアプリケーションでは、言語モデルを使用して入力を解釈し、適切な応答を生成します。

この演習では、Microsoft Foundry を使用して、カスタム データを生成 AI ソリューションに統合します。

> **注**:この演習のコードは、変更される可能性があるプレリリース SDK ソフトウェアに基づいています。 必要に応じて、特定のバージョンのパッケージを使用しました。利用可能な最新バージョンが反映されていない可能性があります。 予期しない動作、警告、またはエラーが発生する場合があります。

この演習は、Azure OpenAI Python SDK に基づいていますが、次のような複数の言語固有の SDK を使用して AI チャット アプリケーションを開発することができます。

- [Python 用の OpenAI](https://pypi.org/project/openai/)
- [Microsoft .NET 用の Azure OpenAI](https://www.nuget.org/packages/Azure.AI.OpenAI)
- [TypeScript 用の Azure OpenAI](https://www.npmjs.com/package/@azure/openai)

この演習は約 **45** 分かかります。

## Microsoft Foundry ハブとプロジェクトを作成する

この演習で使用する Foundry の機能には、Foundry "ハブ" リソースに基づくプロジェクトが必要です。**

1. Web ブラウザーで、[Foundry ポータル](https://ai.azure.com) (`https://ai.azure.com`) を開き、Azure 資格情報を使用してサインインします。 初めてサインインする場合に開かれるヒントまたはクイック スタートのペインを閉じ、必要に応じて、左上にある **[Foundry]** ロゴを使用してホーム ページに移動します。次の図のようなページが表示されます (**[ヘルプ]** ペインが表示される場合は閉じます)。

    ![Foundry ポータルのスクリーンショット。](./media/ai-foundry-home.png)

1. ブラウザーで `https://ai.azure.com/managementCenter/allResources` に移動し、**[新規作成]** を選択します。 次に、新しい **AI ハブ リソース**を作成するオプションを選択します。
1. **[プロジェクトの作成]** ウィザードで、プロジェクトの有効な名前を入力し、新しいハブを作成するオプションを選択します。 次に、**[ハブ名の変更]** リンクを使用して新しいハブの有効な名前を指定し、**[詳細オプション]** を展開し、プロジェクトに次の設定を指定します。
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **リソース グループ**: *リソース グループを作成または選択します*
    - **リージョン**: 米国東部 2 またはスウェーデン中部 (*演習の後半でクォータ制限を超えた場合は、別のリージョンに別のリソースを作成する必要が生じる可能性があります*)。

    > **メモ**: ポリシーを使用して使用可能なリソース名を制限する Azure サブスクリプションで作業している場合は、**[新しいプロジェクトの作成]** ダイアログ ボックスの下部にあるリンクを使用して、Azure portal を使用してハブを作成する必要があります。

    > **ヒント**: **[作成]** ボタンが無効な場合は、ハブの名前を一意の英数字にしてください。

1. プロジェクトが作成されるまで待ってから、プロジェクトに移動します。

## モデルをデプロイする

ソリューションを実装するには、次の 2 つのモデルが必要です。

- 効率的なインデックス作成と処理のためにテキスト データをベクター化する "埋め込み" モデル。**
- ご利用のデータに基づいて、質問に対する自然言語の応答を生成することができるモデル。

1. Foundry ポータル内のプロジェクトで、左側のナビゲーション ウィンドウから **[Model カタログ]** を選択します。
1. フィルター パネルで **[コレクション]** を選択し、**OpenAI** コレクションのみでフィルター処理して `OpenAI` を挿入します。
1. `text-embedding-ada-002` を検索して選択し、詳細ページで **[このモデルを使用します]** を選択します。 購入オプションを含むポップアップが表示された場合は、**[Azure モデルから直接]** オプションを選択します。 
1. モデルの配置ウィザードで **[カスタマイズ]** を選択して、次の設定を使用します。

    - **デプロイ名**: *モデル デプロイの有効な名前*
    - **デプロイの種類**: グローバル標準
    - **モデルのバージョン**: *Select the default version (既定のバージョンの選択)*
    - **接続先 AI リソース**: *以前に作成したリソースを選択します*
    - **1 分あたりのトークンのレート制限 (1,000)**: 50,000 * (または 50,000 未満の場合はサブスクリプションで使用可能な最大値)*
    - **コンテンツ フィルター**: DefaultV2

    > **注**: 現在の AI リソースの場所に、デプロイするモデルで使用可能なクォータがない場合は、新しい AI リソースが作成され、プロジェクトに接続される別の場所を選択するように求められます。

1. **[モデル カタログ]** ページに戻り、前の手順を繰り返し、TPM レート制限が **50K** (または、50K 未満の場合はサブスクリプションで使用可能な最大値) の最新バージョンの **グローバル標準**デプロイを使用して、**gpt-4o** モデルをデプロイします。

    > **注**:1 分あたりのトークン数 (TPM) を減らすと、ご利用のサブスクリプション内で使用可能なクォータが過剰に消費されるのを回避するのに役立ちます。 この演習で使用するデータには、50,000 TPM で十分です。

## プロジェクトにデータを追加する

ご利用のアプリのデータは、架空の旅行代理店 *Margie's Travel* の旅行パンフレット (PDF 形式) のセットで構成されています。 それらをプロジェクトに追加しましょう。

1. 新しいブラウザー タブで、[パンフレットの zip 形式アーカイブ](https://github.com/MicrosoftLearning/mslearn-ai-studio/raw/main/data/brochures.zip)を `https://github.com/MicrosoftLearning/mslearn-ai-studio/raw/main/data/brochures.zip` からダウンロードし、ローカル ファイル システム上の「**brochures**」という名前のフォルダーに展開します。
1. Foundry ポータルでプロジェクトに移動し、左側にあるナビゲーション ウィンドウの **[マイ アセット]** で、**[データとインデックス]** ページを選択します。
1. **[+ New data]\(+ 新しいデータ\)** を選択します。
1. **[データの追加]** ウィザードで、ドロップダウン メニューを展開して **[Upload files/folders]\(ファイル/フォルダーのアップロード\)** を選択します。
1. **[フォルダーのアップロード]** を選択し、**brochures** フォルダーを選択します。 フォルダー内のすべてのファイルが一覧表示されるまで待ちます。
1. **[次へ]** を選択して、データ名を `brochures` に設定します。
1. フォルダーがアップロードされるまで待ちます。これにはいくつかの .pdf ファイルが含まれています。

## データのインデックスを作成する

これで、ご利用のプロジェクトにデータ ソースを追加したので、それを使用して Azure AI 検索リソース内にインデックスを作成することができます。

1. Foundry ポータルでプロジェクトに移動し、左側にあるナビゲーション ウィンドウの **[マイ アセット]** で、**[データとインデックス]** ページを選択します。
1. **[インデックス]** タブで、次の設定で新しいインデックスを追加します。
    - **ソースの場所**:
        - **データ ソース**:Foundry のデータ
            - [データ ソース] で **[brochures]** を選択します**
    - **インデックスの構成**:
        - **Azure AI 検索サービスの選択**: *次の設定で新しい Azure AI 検索リソースを作成します*。
            - **サブスクリプション**: *お使いの Azure サブスクリプション*
            - **リソース グループ**: *お使いの AI ハブと同じリソース グループ*
            - **サービス名**: *お使いの AI 検索リソースの有効な名前*
            - **場所**: *お使いの AI ハブと同じ場所*
            - **価格レベル**: Basic
            
            AI 検索リソースが作成されるまで待ちます。 次に、Foundry に戻り、**[その他の Azure AI 検索リソースへの接続]** を選択し、先ほど作成した AI 検索リソースへの接続を追加して、インデックスの構成を完了します。
 
        - **ベクトル インデックス**: `brochures-index`
        - **仮想マシン**:自動選択
    - **[検索設定]**:
        - **[Vector settings]**:ベクトル検索をこの検索リソースに追加します
        - **Azure OpenAI 接続**: *ハブの既定の Azure OpenAI リソースを選択します。*
        - **埋め込みモデル**: text-embedding-ada-002
        - **埋め込みモデルのデプロイ**: ** text-embedding-ada-002 *モデルのデプロイ*

1. ベクトル インデックスを作成し、そのインデックス作成プロセスが完了するまで待ちます。サブスクリプションで使用可能なコンピューティング リソースによっては、しばらく時間がかかる場合があります。

    このインデックス作成操作は、次のジョブで構成されます。

    - ご利用の brochures データ内でテキスト トークンを分解し、塊に分けて、埋め込みます。
    - Azure AI 検索インデックスを作成します。
    - インデックス資産を登録します。

    > **ヒント**: インデックスが作成されるのを待っている間に、ダウンロードしたパンフレットを見て内容を把握しましょう。

## プレイグラウンドでインデックスをテストする

RAG ベースのプロンプト フローでインデックスを使用する前に、それを使用して生成 AI 応答に作用することができるのを確認しましょう。

1. 左側ナビゲーション ウィンドウ内で、**[プレイグラウンド]** ページを選択し、**[チャット]** プレイグラウンドを開きます。
1. [チャット] プレイグラウンド ページの [セットアップ] ウィンドウで、**GPT-4** モデル デプロイが選択されていることを確認します。 次に、メインの [チャット セッション] パネルで、プロンプト「`Where can I stay in New York?`」を送信します。
1. その応答を確認します。これは、インデックスからのデータを含まない、このモデルからの一般的な回答であるはずです。
1. [セットアップ] ウィンドウで、**"データの追加"** フィールドを展開し、**brochures-index** プロジェクト インデックスを追加して、**[ハイブリッド (ベクトル + キーワード)]** 検索タイプを選択します。

   > **ヒント**: 場合によっては、新しく作成されたインデックスをすぐに使用できない場合があります。 通常はブラウザーの更新で解決しますが、インデックスが見つからない問題がそれでも解決しない場合は、インデックスが認識されるまで待つ必要がある場合があります。

1. インデックスが追加され、チャット セッションが再開された後に、プロンプト「`Where can I stay in New York?`」を再送信します。
1. その応答を確認します。インデックス内のデータに基づいているはずです。

## RAG クライアント アプリを作成する

作業用インデックスが作成されたので、Azure OpenAI SDK を使用して、クライアント アプリケーションに RAG パターンを実装できます。 これを実現するコードを簡単な例で見てみましょう。

### アプリケーション構成を準備する

1. Azure portal を含むブラウザー タブに戻ります (既存のタブでは Foundry ポータルを開いたままにしておきます)。
1. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成し、サブスクリプションにストレージがない ***PowerShell*** 環境を選択します。

    Azure portal の下部にあるペインに Cloud Shell のコマンド ライン インターフェイスが表示されます。 作業しやすくするために、このウィンドウのサイズを変更したり最大化したりすることができます。

    > **注**: *Bash* 環境を使用するクラウド シェルを以前に作成した場合は、それを ***PowerShell*** に切り替えます。

1. Cloud Shell ツール バーの **[設定]** メニューで、**[クラシック バージョンに移動]** を選択します (これはコード エディターを使用するのに必要です)。

    **<font color="red">続行する前に、クラシック バージョンの Cloud Shell に切り替えたことを確認します。</font>**

1. Cloud Shell 画面で、次のコマンドを入力して、この演習のコード ファイルを含む GitHub リポジトリをクローンします (コマンドを入力するか、クリップボードにコピーしてから、コマンド ラインで右クリックし、プレーンテキストとして貼り付けます)。

    ```
    rm -r mslearn-ai-foundry -f
    git clone https://github.com/microsoftlearning/mslearn-ai-studio mslearn-ai-foundry
    ```

    > **ヒント**: Cloudshell にコマンドを貼り付けると、出力が大量のスクリーン バッファーを占有する可能性があります。 `cls` コマンドを入力して、各タスクに集中しやすくすることで、スクリーンをクリアできます。

1. リポジトリが複製されたら、チャット アプリケーションのコード ファイルを含んだフォルダーに移動します。

    ```
   cd mslearn-ai-foundry/labfiles/rag-app/python
    ```

1. Cloud Shell コマンド ライン ペインで、次のコマンドを入力して、OpenAI SDK ライブラリをインストールします。

    ```
   python -m venv labenv
   ./labenv/bin/Activate.ps1
   pip install -r requirements.txt openai
    ```

1. 次のコマンドを入力して、提供されている構成ファイルを編集します。

    ```
   code .env
    ```

    このファイルをコード エディターで開きます。

1. 設定ファイルで次のプレースホルダーを置き換えます。 
    - **your_openai_endpoint**: Foundry ポータルのプロジェクトの **[概要]** ページからの Open AI エンドポイント (必ず **[Azure OpenAI]** 機能タブを選択してください)。
    - **your_openai_api_key**: Foundry ポータルのプロジェクトの **[概要]** ページからの Open AI API キー (必ず **[Azure OpenAI]** 機能タブを選択してください)。
    - **your_chat_model**: Foundry ポータルの **[モデルとエンドポイント]** ページで **gpt-4o** モデル デプロイに割り当てた名前 (既定の名前は `gpt-4o`)。
    - **your_embedding_model**: Foundry ポータルの **[モデルとエンドポイント]** ページで **text-embedding-ada-002** モデル デプロイに割り当てた名前 (既定の名前は `text-embedding-ada-002`)。
    - **your_search_endpoint**: Azure AI 検索リソースの URL。 これは、Foundry ポータルの **[管理センター]** にあります。
    - **your_search_api_key**: Azure AI 検索リソースの API キー。 これは、Foundry ポータルの **[管理センター]** にあります。
    - **your_index**: Foundry ポータルのプロジェクトの **[データとインデックス]** ページからのインデックス名に置き換えます (これは `brochures-index` にする必要があります)。
1. プレースホルダーを置き換えたら、コード エディター内で、**Ctrl + S** コマンドを使用するか、**右クリックして保存**で変更を保存してから、**Ctrl + Q** コマンドを使用するか、**右クリックして終了**で、Cloud Shell コマンド ラインを開いたままコード エディターを閉じます。

### RAG パターンを実装するコードを調べる

1. 次のコマンドを入力して、提供されているコード ファイルを編集します。

    ```
   code rag-app.py
    ```

1. ファイル内のコードを確認し、次の点に注意します。
    - エンドポイント、キー、チャット モデルを使用して Azure OpenAI クライアントを作成します。
    - 旅行関連のチャット ソリューションに適したシステム メッセージを作成します。
    - プロンプト (ユーザーによる入力に基づいたシステムおよびユーザー メッセージを含む) を Azure OpenAI クライアントに送信し、次を追加します。
        - クエリを実行する Azure AI Search インデックスの接続の詳細。
        - クエリのベクター化に使用する埋め込みモデルの詳細\*。
    - グラウンディングされたプロンプトからの応答を表示します。
    - チャット履歴に応答を追加します。

    \**検索インデックスのクエリはプロンプトに基づいており、インデックス付きドキュメント内の関連するテキストを検索するために使用されます。クエリをテキストとして送信するキーワードベースの検索を使用できますが、ベクターベースの検索を使用する方が効率的です。そのため、埋め込みモデルを使用してクエリ テキストを送信する前にベクター化することができます。*

1. **Ctrl + Q** コマンドを使用して、Cloud Shell コマンド ラインを開いたまま、変更を保存せずにコード エディターを閉じます。

### チャット アプリケーションを実行する

1. Cloud Shell コマンド ライン ペインで、次のコマンドを入力してアプリを実行します。

    ```
   python rag-app.py
    ```

1. メッセージが表示されたら、`Where should I go on vacation to see architecture?` などの質問を入力し、生成 AI モデルからの応答を確認します。

    なお、応答には、回答が見つかったインデックス付きデータを示すソース参照が含まれています。

1. フォローアップの質問を試します (例: `Where can I stay there?`)。

1. 終了したら、`quit` を入力してプログラムを終了します。 次に、Cloud Shell 画面を閉じます。

## クリーンアップ

不要な Azure のコストとリソース使用を回避するには、この演習でデプロイしたリソースを削除する必要があります。

1. Foundry を調べ終わったら、[Azure portal](https://portal.azure.com) (`https://portal.azure.com`) に戻ります。必要に応じて、自分の Azure 資格情報を使用してサインインします。 次に、Azure AI 検索と Azure AI リソースをプロビジョニングしたリソース グループ内のリソースを削除します。
