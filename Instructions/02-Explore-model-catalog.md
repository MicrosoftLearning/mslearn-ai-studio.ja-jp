---
lab:
  title: Azure AI Studio で言語モデルを使用して探索、デプロイ、チャットを行う
---

# Azure AI Studio で言語モデルを使用して探索、デプロイ、チャットを行う

Azure AI Studio のモデル カタログは、さまざまなモデルを試してから使用できる中央リポジトリとして機能し、生成 AI シナリオの作成を容易にします。

この演習では、Azure AI Studio でモデル カタログをいろいろ試すことができます。

この演習には、約 **25** 分かかります。

## Azure AI ハブを作成する

プロジェクトをホストするには、Azure サブスクリプション内に Azure AI ハブが必要です。 このリソースは、プロジェクトの作成中に作成するか、事前にプロビジョニングすることができます (この演習ではこちらを行います)。

1. **[管理]** セクションで、**[すべてのハブ]** を選択した後、**[+ 新しいハブ]** を選択します。 次の設定を使用して新しいハブを作成します。
    - **ハブ名**:*一意の名前*
    - **[サブスクリプション]**:"*ご自身の Azure サブスクリプション*"
    - **[リソース グループ]**: "一意の名前で新しいリソース グループを作成するか、既存のものを選びます"**
    - **[場所]**: *以下のいずれかのリージョンから**ランダム**に選択する*\*
        - オーストラリア東部
        - カナダ東部
        - 米国東部
        - 米国東部 2
        - フランス中部
        - 東日本
        - 米国中北部
        - スウェーデン中部
        - スイス北部
        - 英国南部
    - **Azure AI サービスまたは Azure OpenAI への接続**:新しい AI サービスを作成するか既存のものを使用するかを選択する**
    - **Azure AI 検索への接続**:接続をスキップする

    > \* Azure OpenAI リソースは、リージョンのクォータによってテナント レベルで制限されます。 一覧表示されているリージョンには、この演習で使用されるモデル タイプの既定のクォータが含まれています。 リージョンをランダムに選択すると、テナントを他のユーザーと共有するシナリオで、1 つのリージョンがクォータ制限に達するリスクが軽減されます。 演習の後半でクォータ制限に達した場合は、別のリージョンに別のリソースを作成する必要が生じる可能性があります。

    Azure AI ハブが作成されると、次の画像のようになるはずです。

    ![Azure AI Studio 内の Azure AI ハブの詳細のスクリーンショット。](./media/azure-ai-resource.png)

1. 新しいブラウザー タブを開き ([Azure AI Studio] タブは開いたままにします)、Azure portal ([https://portal.azure.com](https://portal.azure.com?azure-portal=true)) に移動します。プロンプトが表示されたら、Azure の資格情報を使ってサインインします。
1. Azure AI ハブを作成したリソース グループに移動し、作成された Azure リソースを表示します。

    ![Azure portal 内の Azure AI ハブと関連リソースのスクリーンショット。](./media/azure-portal.png)

1. Azure AI Studio ブラウザー タブに戻ります。
1. Azure AI ハブのページの左側のペインでそれぞれのページを表示し、作成および管理できる成果物を確認します。 **[接続]** ページで、Azure OpenAI と AI サービスへの接続が既に作成されていることを確認します。

## プロジェクトの作成

Azure AI ハブは、1 つ以上の "プロジェクト" を定義できる共同作業用のワークスペースとして機能します。** Azure AI ハブ内にプロジェクトを作成してみましょう。

1. Azure AI Studio で、先ほど作成したハブ内にいることを確認します (画面の上部にあるパスを確認することで、自分の場所を確認できます)。
1. 左側のメニューを使用して、**[すべてのプロジェクト]** に移動します。
1. **[+ New project]** を選択します。
1. **[新しいプロジェクトの作成]** ウィザードで、次の設定を使ってプロジェクトを作成します。
    - **現在のハブ**:あなたの AI ハブ**
    - **プロジェクト名**:"プロジェクトの一意の名前"**
1. プロジェクトが作成されるまで待ちます。 準備ができたら、次の画像のようになります。

    ![Azure AI Studio のプロジェクト詳細ページのスクリーンショット。](./media/azure-ai-project.png)

1. 左側のペインにあるページを表示して各セクションを展開し、プロジェクト内で実行できるタスクと管理できるリソースを確認します。

## モデル ベンチマークを使用してモデルを選択する

モデルをデプロイする前に、モデル ベンチマークを調べて、ニーズに最適なモデルを決定できます。

旅行アシスタントとして機能するカスタム Copilot を作成するとします。 具体的には、Copilot がビザの要件、天気予報、地元の観光スポット、文化的規範などの旅行関連の問い合わせのサポートをするようにします。

Copilot は事実に基づく正確な情報を提供する必要があるため、根拠性が重要です。 その次に、Copilot の回答が読みやすく理解しやすいものにする必要があります。 したがって、流暢さと一貫性に対して高い評価のモデルを選択する必要もあります。

1. Azure AI Studio で、左側のメニューを使用して、**[開始]** セクションの **[モデル ベンチマーク]** に移動します。
    **[品質ベンチマーク]** タブには、いくつかのグラフが既に視覚化されており、さまざまなモデルを比較できます。
1. 表示されているモデルのフィルター処理:
    - **[タスク]**: 質問応答
    - **[コレクション]**: Azure OpenAI
    - **[メトリック]**: 一貫性、流暢さ、根拠性
1. 結果のグラフと比較テーブルを確認します。 確認の際には、次の質問に答えるとよいでしょう。
    - GPT-3.5 モデルと GPT-4 モデルのパフォーマンスの違いに気付きましたか?
    - 同じモデルのバージョン間に違いはありますか?
    - 32k バリアントはベースモデルとどう異なりますか?

Azure OpenAI コレクションから、GPT-3.5 モデルと GPT-4 モデルを選択できます。 これら 2 つのモデルをデプロイし、実際のユース ケースでどう異なるか調べてみましょう。

## Azure OpenAI モデルのデプロイ

モデル ベンチマークを使用したオプションを見てきたので、言語モデルをデプロイする準備ができました。 モデル カタログを参照してそこからデプロイすることも、**[デプロイ]** ページを使用してモデルをデプロイすることもできます。 両方のオプションを見てみましょう。

### モデル カタログからモデルをデプロイする

まず、モデル カタログからモデルをデプロイします。 使用可能なすべてのモデルをフィルター処理する場合には、このオプションが好ましい場合があります。

1. 左側のメニューを使用して、**[開始]** セクションの **[モデル カタログ]** ページに移動します。
1. 次の設定を使用して、Azure AI によってキュレーションされた `gpt-35-turbo` モデルを検索してデプロイします。
    - **[デプロイ名]**: *モデル デプロイの一意の名前、GPT-3.5 モデルであることを示します*
    - **モデルのバージョン**: *Select the default version (既定のバージョンの選択)*
    - **デプロイの種類**:Standard
    - **Azure OpenAI リソースに接続済み**:ハブの作成時に作成された既定の接続を選択する**
    - **1 分あたりのトークン数のレート制限 (1,000 単位)**:5,000
    - **コンテンツ フィルター**: 既定

### [デプロイ] でモデルをデプロイする

デプロイするモデルが既にわかっている場合は、[デプロイ] でそれを行うことができます。

1. 左側のメニューを使用して、**[コンポーネント]** セクションの **[デプロイ]** ページに移動します。
1. **[モデル デプロイ]** タブで、次の設定を使用して新しいデプロイを作成します。
    - **[モデル]**: gpt-4
    - **[デプロイ名]**: *モデル デプロイの一意の名前、GPT-4 モデルであることを示します*
    - **モデルのバージョン**: *Select the default version (既定のバージョンの選択)*
    - **デプロイの種類**:Standard
    - **Azure OpenAI リソースに接続済み**:ハブの作成時に作成された既定の接続を選択する**
    - **1 分あたりのトークン数のレート制限 (1,000 単位)**:5,000
    - **コンテンツ フィルター**: 既定

    > **注**: モデル カタログのオプションとしてではなく、モデル ベンチマークが表示されているモデルがあることに気付いたかもしれません。 モデルの可用性は場所によって異なります。 場所は、デプロイするモデルを**場所ヘルパー**により指定し、デプロイできる場所の一覧を取得できる、AI ハブ レベルで指定されます。

## チャット プレイグラウンドでモデルをテストする

比較できるモデルが 2 つあるので、対話的操作でモデルがどのように動作するかを見てみましょう。

1. 左側のメニューを使用して、**[プロジェクト プレイグラウンド]** セクションの **[チャット]** ページに移動します。
1. **[チャット プレイグラウンド]** で、GPT-3.5 デプロイを選択します。
1. チャット ウィンドウで、「`What can you do?`」という質問を入力し、応答を確認します。
    答えは非常に一般的です。 旅行アシスタントとして機能するカスタムの Copilot を作成しようとしていることを思い出してください。 質問において、求めるサポートの種類を指定できます。
1. チャット ウィンドウで、「`Imagine you're a travel assistant, what can you help me with?`」という質問を入力します。回答はより具体的になりました。 エンド ユーザーが Copilot と対話するたびに必要なコンテキストを提供しなくて済むようにしたい場合があるでしょう。 包括的な指示を追加するには、システム・メッセージを編集することができます。
1. 次のプロンプトでシステム メッセージを更新します。

   ```
   You are an AI travel assistant that helps people plan their trips. Your objective is to offer support for travel-related inquiries, such as visa requirements, weather forecasts, local attractions, and cultural norms.
   ```

1. **[変更の適用]**、**[チャットのクリア]** の順に選択します。
1. チャット ウィンドウで、「`What can you do?`」というクエリを入力し、新しい応答を確認します。 前に受け取った回答とどう異なるかを確認します。 回答は旅行に限定されたものになりました。
1. 「`I'm planning a trip to London, what can I do there?`」と質問して会話を続けます。Copilot は旅行関連情報をたくさん提供します。 出力をさらに改善したい場合があるでしょう。 たとえば、回答をより簡潔にしたい場合があります。
1. メッセージの最後に「`Answer with a maximum of two sentences.`」を追加して、システム メッセージを更新します。 変更を適用し、チャットをクリアして、「`I'm planning a trip to London, what can I do there?`」という質問でもう一度チャットをテストします。Copilot が、質問に答えるだけはなく、会話を続けるようにもできます。
1. メッセージの最後に「`End your answer with a follow-up question.`」を追加して、システム メッセージを更新します。 変更を適用し、チャットをクリアして、「`I'm planning a trip to London, what can I do there?`」という質問でもう一度チャットをテストします。
1. **[デプロイ]** を GPT-4 モデルに変更し、このセクションのすべての手順を繰り返します。 モデルの出力が異なる場合があることに注意してください。
1. 最後に、「`Who is the prime minister of the UK?`」という質問で両方のモデルをテストします。 この質問のパフォーマンスは、モデルの根拠性 (応答が事実について正確かどうか) に関連しています。 パフォーマンスは、モデル ベンチマークでの結論と関連していますか?

両方のモデルを調べたので、実際のユース ケースに対して今どのモデルを選択するか検討します。 最初は、モデルからの出力が異なる場合があり、一方のモデルが別のモデルよりも良いと考えるかもしれません。 ただし、システム メッセージを更新した後では、違いが非常に小さいことに気付くでしょう。 両者のパフォーマンスが非常に近いため、コスト最適化の観点から、GPT-4 モデルよりも GPT-3.5 モデルを選択するかもしれません。

## クリーンアップ

Azure AI Studio を確認し終わったら、不要な Azure コストが発生しないように、この演習で作成したリソースを削除する必要があります。

1. Azure portal が表示されているブラウザー タブに戻り (または、新しいブラウザー タブで [Azure portal](https://portal.azure.com?azure-portal=true) をもう一度開き)、この演習で使ったリソースがデプロイされているリソース グループの内容を表示します。
1. ツール バーの **[リソース グループの削除]** を選びます。
1. リソース グループ名を入力し、削除することを確認します。
